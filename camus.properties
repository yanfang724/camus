#Top-level data output directory, sub-directories will be dynamically created for each topic pulled
etl.destination.path=/user/ballarde/camus/output
#HDFS location where you want to keep execution files, i.e. offsets, error logs, and count files
etl.execution.base.path=/user/ballarde/camus/exec
#Where completed Camus job output directories are kept, usually a sub-dir in the base.path
etl.execution.history.path=/user/ballarde/camus/exec/history
#Zookeeper configurations:
zookeeper.hosts=scajbda10 	
zookeeper.broker.topics=/brokers/topics
zookeeper.broker.nodes=/brokers/ids
#All files in this dir will be added to the distributed cache and placed on the classpath for hadoop tasks
hdfs.default.classpath.dir=
#Max hadoop tasks to use, each task can pull multiple topic partitions
mapred.map.tasks=30
#Max historical time that will be pulled from each partition based on event timestamp
kafka.max.pull.hrs=1
#Events with a timestamp older than this will be discarded.
kafka.max.historical.days=3
#Max minutes for each mapper to pull messages
kafka.max.pull.minutes.per.task=-1
#Decoder class for Kafka Messages to Avro Records
camus.message.decoder.class=com.linkedin.camus.etl.kafka.coders.MyDecoder
etl.record.writer.provider.class=com.linkedin.camus.etl.kafka.common.StringRecordWriterProvider

#If whitelist has values, only whitelisted topic are pulled. Nothing on the blacklist is pulled
#kafka.blacklist.topics=
#kafka.whitelist.topics=

#kafka contact info
kafka.host.url=scaj22bda17
kafka.host.port=9092

#??
hdfs.default.classpath.dir=/user/ballarde/temp/kafka/lib

# Name of the client as seen by kafka
kafka.client.name=camus

kafka.client.buffer.size=20971520
kafka.client.so.timeout=60000